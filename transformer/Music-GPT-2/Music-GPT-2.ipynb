{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import ntpath\n",
    "\n",
    "data_dirs = ['dataset_cc', 'dataset_cc_midi']\n",
    "\n",
    "data_files = []\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    data_files += [join(data_dir, f) for f in listdir(data_dir) if isfile(join(data_dir, f)) if '.npz' in f]\n",
    "print(len(data_files))\n",
    "\n",
    "data_files.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntervalDim = 100\n",
    "\n",
    "VelocityDim = 32\n",
    "VelocityOffset = IntervalDim\n",
    "\n",
    "NoteOnDim = NoteOffDim = 128\n",
    "NoteOnOffset = IntervalDim + VelocityDim\n",
    "NoteOffOffset = IntervalDim + VelocityDim + NoteOnDim\n",
    "\n",
    "CCDim = 2\n",
    "CCOffset = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim + CCDim # 390\n",
    "\n",
    "Time = 2000\n",
    "\n",
    "EmbeddingDim = 512\n",
    "\n",
    "HeadDim = 32\n",
    "Heads = 16\n",
    "ContextDim = HeadDim * Heads # 512\n",
    "\n",
    "Layers = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "def default_hparams():\n",
    "    return HParams(\n",
    "        n_vocab=EventDim,\n",
    "        n_ctx=ContextDim,\n",
    "        n_embd=EmbeddingDim,\n",
    "        n_head=Heads,\n",
    "        n_layer=Layers,\n",
    "        n_time=Time,\n",
    "    )\n",
    "\n",
    "hparams = default_hparams()\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from npz and converter to token sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "def get_data(length=Time):\n",
    "    index = np.random.randint(0, len(data_files))\n",
    "    data = np.load(data_files[index])['eventlist']\n",
    "    \n",
    "    # time augmentation\n",
    "    data[:, 0] *= np.random.uniform(0.80, 1.20)\n",
    "    \n",
    "    # absolute time to relative interval\n",
    "    data[1:, 0] = data[1:, 0] - data[:-1, 0]\n",
    "    data[0, 0] = 0\n",
    "    \n",
    "    # discretize interval into IntervalDim\n",
    "    data[:, 0] = np.clip(np.round(data[:, 0] * IntervalDim), 0, IntervalDim - 1)\n",
    "    \n",
    "    # Note augmentation\n",
    "    data[:, 2] += np.random.randint(-6, 6)\n",
    "    data[:, 2] = np.clip(data[:, 2], 0, NoteOnDim - 1)\n",
    "    \n",
    "    eventlist = []\n",
    "    for d in data:\n",
    "        # append interval\n",
    "        interval = d[0]\n",
    "        eventlist.append(interval)\n",
    "    \n",
    "        # note on case\n",
    "        if d[1] == 1:\n",
    "            velocity = (d[3] / 128) * VelocityDim + VelocityOffset\n",
    "            note = d[2] + NoteOnOffset\n",
    "            eventlist.append(velocity)\n",
    "            eventlist.append(note)\n",
    "            \n",
    "        # note off case\n",
    "        elif d[1] == 0:\n",
    "            note = d[2] + NoteOffOffset\n",
    "            eventlist.append(note)\n",
    "        # CC\n",
    "        elif d[1] == 2:\n",
    "            event = CCOffset + d[3]\n",
    "            eventlist.append(event)\n",
    "            \n",
    "    eventlist = np.array(eventlist).astype(np.int)\n",
    "    \n",
    "    if len(eventlist) > (length+1):\n",
    "        start_index = np.random.randint(0, len(eventlist) - (length+1))\n",
    "        eventlist = eventlist[start_index:start_index+(length+1)]\n",
    "        \n",
    "    # pad zeros\n",
    "    if len(eventlist) < (length+1):\n",
    "        pad = (length+1) - len(eventlist)\n",
    "        eventlist = np.pad(eventlist, (pad, 0), 'constant')\n",
    "        \n",
    "    x = eventlist[:length]\n",
    "    y = eventlist[1:length+1]\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "x, y = get_data()\n",
    "print('x shape : ', x.shape)\n",
    "print('y shape : ', y.shape)\n",
    "# print(x)\n",
    "# print(y)\n",
    "    \n",
    "    \n",
    "roll = np.zeros([len(x), EventDim])\n",
    "for t, _x in enumerate(x):\n",
    "    roll[t, _x] = 1\n",
    "\n",
    "plt.figure(figsize=[18, 15])\n",
    "librosa.display.specshow(roll.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 source code from https://github.com/openai/gpt-2/blob/master/src/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n",
    "    ex = tf.exp(x)\n",
    "    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        n_state = x.shape[-1].value\n",
    "        g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n",
    "        b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n",
    "        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
    "        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "'''\n",
    "MEMORY EFFICIENT IMPLEMENTATION OF RELATIVE POSITION-BASED ATTENTION\n",
    "(Music Transformer, Cheng-Zhi Anna Huang et al. 2018)\n",
    "'''\n",
    "def attn(x, scope, n_state, *, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "    \n",
    "    def relative_attn(q):\n",
    "        # q have shape [batch, heads, sequence, features]\n",
    "        batch, heads, sequence, features = shape_list(q)\n",
    "        E = tf.get_variable('E', [heads, sequence, features])\n",
    "        # [heads, batch, sequence, features]\n",
    "        q_ = tf.transpose(q, [1, 0, 2, 3])\n",
    "        # [heads, batch * sequence, features]\n",
    "        q_ = tf.reshape(q_, [heads, batch * sequence, features])\n",
    "        # [heads, batch * sequence, sequence]\n",
    "        rel = tf.matmul(q_, E, transpose_b=True)\n",
    "        # [heads, batch, sequence, sequence]\n",
    "        rel = tf.reshape(rel, [heads, batch, sequence, sequence])\n",
    "        # [heads, batch, sequence, 1+sequence]\n",
    "        rel = tf.pad(rel, ((0, 0), (0, 0), (0, 0), (1, 0)))\n",
    "        # [heads, batch, sequence+1, sequence]\n",
    "        rel = tf.reshape(rel, (heads, batch, sequence+1, sequence))\n",
    "        # [heads, batch, sequence, sequence]\n",
    "        rel = rel[:, :, 1:]\n",
    "        # [batch, heads, sequence, sequence]\n",
    "        rel = tf.transpose(rel, [1, 0, 2, 3])\n",
    "        return rel\n",
    "        \n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w + relative_attn(q)\n",
    "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = softmax(w)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = x.shape[-1].value\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "\n",
    "def block(x, scope, *, hparams):\n",
    "    with tf.variable_scope(scope):\n",
    "        nx = x.shape[-1].value\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def model(hparams, X, scope='model', reuse=False):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        h = tf.gather(wte, X)\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        for layer in range(hparams.n_layer):\n",
    "            h, present = block(h, 'h%d' % layer, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Main Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = default_hparams()\n",
    "print(hparams)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, hparams.n_time])\n",
    "Y = tf.placeholder(tf.int32, [None, hparams.n_time])\n",
    "\n",
    "X_onehot = tf.one_hot(X, axis=2, depth=hparams.n_vocab)\n",
    "\n",
    "logits = model(hparams, X)['logits']\n",
    "probs = tf.nn.softmax(logits, axis=2)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#temperature = tf.Variable(1., name='temperature')\n",
    "temperature = 0\n",
    "u = tf.random.uniform(shape=tf.shape(logits[:, -1]), minval=1e-5, maxval=1.-1e-5)\n",
    "u = (logits[:, -1] - tf.log(temperature + 1e-8)) - tf.log(-tf.log(u))\n",
    "sample = tf.argmax(u, axis=1)\n",
    "\n",
    "#dist = tf.distributions.Categorical(logits=logits[:, -1])\n",
    "#sample = dist.sample()\n",
    "\n",
    "'''\n",
    "Train\n",
    "'''\n",
    "global_step = tf.Variable(0, name='global_step')\n",
    "learning_rate = tf.Variable(1e-3, name='learning_rate')\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "\n",
    "'''\n",
    "Session Open\n",
    "'''\n",
    "\n",
    "\n",
    "# GPU number to use\n",
    "gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "# sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('graph create')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model if exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "\n",
    "load_dir = 'save/gpt2-cc-interval100-attention2000-midi'\n",
    "save_dir = 'save/gpt2-cc-interval100-attention2000-midi'\n",
    "\n",
    "def get_variables_from_checkpoint_file(file_name):\n",
    "    variables = []\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n",
    "\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "    for key in sorted(var_to_shape_map):\n",
    "        variables.append((key, var_to_shape_map[key]))\n",
    "\n",
    "    return variables\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if True:\n",
    "    restore_file = tf.train.latest_checkpoint(load_dir)\n",
    "    if restore_file is not None:\n",
    "        try:\n",
    "            saver.restore(sess, restore_file)\n",
    "            print(\"Model restored.\", restore_file)\n",
    "        except:\n",
    "            saved_variables = get_variables_from_checkpoint_file(restore_file)\n",
    "            model_variables = slim.get_variables_to_restore()\n",
    "            restore_variables = []\n",
    "            for model_variable in model_variables:\n",
    "                for saved_variable_name, saved_variable_shape in saved_variables:\n",
    "                    model_variable_name = model_variable.name.split(\":\")[0]\n",
    "                    if saved_variable_name == model_variable_name and tuple(saved_variable_shape) == model_variable.shape:\n",
    "                        restore_variables.append(model_variable)\n",
    "\n",
    "            init_saver = tf.train.Saver(restore_variables)\n",
    "            init_saver.restore(sess, restore_file)\n",
    "            print(\"Model partially restored.\")\n",
    "    else:\n",
    "        print('model not exist.')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorboardX Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class Logger(SummaryWriter):\n",
    "    def __init__(self, logdir):\n",
    "        super(Logger, self).__init__(logdir)\n",
    "\n",
    "    def log(self, log_string, value, iteration):\n",
    "            self.add_scalar(log_string, value, iteration)\n",
    "            \n",
    "logger = Logger(save_dir)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "while(True):\n",
    "    for _ in range(100):\n",
    "        _inputs = []\n",
    "        _targets = []\n",
    "        for _ in range(batch_size):\n",
    "            while(True):\n",
    "                x, y = get_data(hparams.n_time)\n",
    "                if(x.shape == y.shape):\n",
    "                    break\n",
    "                 \n",
    "            _inputs.append(x)\n",
    "            _targets.append(y)\n",
    "        _inputs = np.stack(_inputs)\n",
    "        _targets = np.stack(_targets)\n",
    "        print(_inputs.shape, _targets.shape)\n",
    "        \n",
    "        _, _global_step, _loss = sess.run([train_step, global_step, loss], \n",
    "                                          feed_dict={X: _inputs, \n",
    "                                                     Y: _targets,\n",
    "                                                     learning_rate: 1e-3})\n",
    "        print(_global_step, _loss)\n",
    "        \n",
    "        if _global_step % 10 == 0:\n",
    "            logger.log('loss', _loss, _global_step)\n",
    "        \n",
    "        if _global_step % 1000 == 0:\n",
    "            save_path = saver.save(sess, save_dir + '/checkpoint', global_step=_global_step)\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "    clear_output()\n",
    "    \n",
    "    _inputs_onehot, _probs = sess.run([X_onehot, probs], feed_dict={X: _inputs})\n",
    "    \n",
    "    plt.figure(figsize=[18, 8])\n",
    "    librosa.display.specshow(_inputs_onehot[0].T)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=[18, 8])\n",
    "    librosa.display.specshow(_probs[0].T)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output MIDI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "\n",
    "load_dir = 'save/gpt2-cc-interval100-attention2000-midi'\n",
    "save_dir = 'save/gpt2-cc-interval100-attention2000-midi'\n",
    "\n",
    "def get_variables_from_checkpoint_file(file_name):\n",
    "    variables = []\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n",
    "\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "    for key in sorted(var_to_shape_map):\n",
    "        variables.append((key, var_to_shape_map[key]))\n",
    "\n",
    "    return variables\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if True:\n",
    "    restore_file = tf.train.latest_checkpoint(load_dir)\n",
    "    if restore_file is not None:\n",
    "        try:\n",
    "            saver.restore(sess, restore_file)\n",
    "            print(\"Model restored.\", restore_file)\n",
    "        except:\n",
    "            saved_variables = get_variables_from_checkpoint_file(restore_file)\n",
    "            model_variables = slim.get_variables_to_restore()\n",
    "            restore_variables = []\n",
    "            for model_variable in model_variables:\n",
    "                for saved_variable_name, saved_variable_shape in saved_variables:\n",
    "                    model_variable_name = model_variable.name.split(\":\")[0]\n",
    "                    if saved_variable_name == model_variable_name and tuple(saved_variable_shape) == model_variable.shape:\n",
    "                        restore_variables.append(model_variable)\n",
    "\n",
    "            init_saver = tf.train.Saver(restore_variables)\n",
    "            init_saver.restore(sess, restore_file)\n",
    "            print(\"Model partially restored.\")\n",
    "    else:\n",
    "        print('model not exist.')\n",
    "\n",
    "N = 6000\n",
    "\n",
    "x, y = get_data(hparams.n_time)\n",
    "_inputs = np.zeros([1, N], dtype=np.int32)\n",
    "_inputs[:, :len(x)] = x[None, :]\n",
    "print(_inputs)\n",
    "\n",
    "for i in tqdm(range(N-Time)):\n",
    "\n",
    "    _sample, _prob = sess.run([sample, probs], feed_dict={X: _inputs[:, i:i+Time]})\n",
    "    _inputs[:, i+Time] = _sample \n",
    "\n",
    "print(_inputs.shape)\n",
    "\n",
    "class Event():\n",
    "    def __init__(self, time, note, cc, on, velocity):\n",
    "        self.time = time\n",
    "        self.note = note\n",
    "        self.on = on\n",
    "        self.cc = cc\n",
    "        self.velocity = velocity\n",
    "\n",
    "    def get_event_sequence(self):\n",
    "        return [self.time, self.note, int(self.on)]\n",
    "\n",
    "class Note():\n",
    "    def __init__(self):\n",
    "        self.pitch = 0\n",
    "        self.start_time = 0\n",
    "        self.end_time = 0\n",
    "\n",
    "event_list = []\n",
    "time = 0\n",
    "event = None\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim # 388\n",
    "\n",
    "for _input in _inputs[0]:\n",
    "    # interval\n",
    "    if _input < IntervalDim: \n",
    "        time += _input\n",
    "        event = Event(time, 0, False, 0, 0)\n",
    "\n",
    "    # velocity\n",
    "    elif _input < NoteOnOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.velocity = (_input - VelocityOffset) / VelocityDim * 128\n",
    "        #print('velocity : ', event.velocity)\n",
    "\n",
    "    # note on\n",
    "    elif _input < NoteOffOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "\n",
    "        event.note = _input - NoteOnOffset\n",
    "        event.on = True\n",
    "        event_list.append(event)\n",
    "        #event_list.append(Event(event.time + 100, event.note, False))\n",
    "        event = None\n",
    "\n",
    "    # note off\n",
    "    elif _input < CCOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.note = _input - NoteOffOffset\n",
    "        event.on = False\n",
    "        event_list.append(event)\n",
    "        event = None\n",
    "\n",
    "    ## CC\n",
    "    else:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.cc = True\n",
    "        on = _input - CCOffset == 1\n",
    "        event.on = on\n",
    "        #print(on)\n",
    "        event_list.append(event)\n",
    "        event = None\n",
    "\n",
    "import midi\n",
    "# Instantiate a MIDI Pattern (contains a list of tracks)\n",
    "pattern = midi.Pattern()\n",
    "# Instantiate a MIDI Track (contains a list of MIDI events)\n",
    "track = midi.Track()\n",
    "# Append the track to the pattern\n",
    "pattern.append(track)\n",
    "\n",
    "prev_time = 0\n",
    "pitches = [None for _ in range(128)]\n",
    "for event in event_list:\n",
    "    tick = (event.time - prev_time) * 5\n",
    "    prev_time = event.time\n",
    "\n",
    "    # case NOTE:\n",
    "    if not event.cc:\n",
    "        if event.on:\n",
    "            if pitches[event.note] is not None:\n",
    "                # Instantiate a MIDI note off event, append it to the track\n",
    "                off = midi.NoteOffEvent(tick=0, pitch=event.note)\n",
    "                track.append(off)\n",
    "                pitches[event.note] = None\n",
    "\n",
    "            # Instantiate a MIDI note on event, append it to the track\n",
    "            on = midi.NoteOnEvent(tick=tick, velocity=int(event.velocity), pitch=event.note)\n",
    "            track.append(on)\n",
    "            pitches[event.note] = prev_time\n",
    "        else:\n",
    "            # Instantiate a MIDI note off event, append it to the track\n",
    "            off = midi.NoteOffEvent(tick=tick, pitch=event.note)\n",
    "            track.append(off)\n",
    "            pitches[event.note] = None\n",
    "\n",
    "    # case CC:\n",
    "    elif event.cc:\n",
    "        if event.on:\n",
    "            cc = midi.ControlChangeEvent(tick=tick, control=64, value=64)\n",
    "        else:\n",
    "            cc = midi.ControlChangeEvent(tick=tick, control=64, value=0)\n",
    "\n",
    "        track.append(cc)\n",
    "\n",
    "    for pitch in range(128):\n",
    "        if pitches[pitch] is not None and pitches[pitch] + 100 < prev_time:\n",
    "            #print('here')\n",
    "            off = midi.NoteOffEvent(tick=0, pitch=pitch)\n",
    "            track.append(off)\n",
    "            pitches[pitch] = None\n",
    "\n",
    "\n",
    "# Add the end of track event, append it to the track\n",
    "eot = midi.EndOfTrackEvent(tick=1)\n",
    "track.append(eot)\n",
    "# Print out the pattern\n",
    "#print(pattern)\n",
    "# Save the pattern to disk\n",
    "midi.write_midifile(\"output_file.mid\", pattern)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
